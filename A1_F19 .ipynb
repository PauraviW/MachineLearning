{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fzMyE386oaaV"
   },
   "source": [
    "**Fall 2019**\n",
    "\n",
    "**P556: Applied Machine Learning**\n",
    "\n",
    "**Assignment #1**\n",
    "\n",
    "**Due date: September 18, 2019. 11:59 PM**\n",
    "\n",
    "DO NOT CHANGE THE FUNCTION DEFINITIONS UNLESS APPROVED BY AN AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJz8ZxU7opMy"
   },
   "source": [
    "# Problem #1: Linear Regression\n",
    "\n",
    "##  Problem 1.1 (25 points)\n",
    "\n",
    "Implement linear regression using gradient descent. Your implementation should be able to handle simple and multiple linear regression.\n",
    "\n",
    "Note 1: by implementation we mean that everything has to be written from scratch and that you cannot call a linear regression function from a library, such as sklearn. Usage of standard libraries, such as numpy, pandas, etc., is fine. If you are unsure about whether a library can be used, please contact the AIs well in advance of the submission date.\n",
    "\n",
    "Note 2: You are free to use sklearn to test whether your results match that from a battle-tested library. This is a great way to know before hand whether your submission is correct. Make sure to use the same parameters on both models before you spend an eternity debugging code that is correct but not returning the same values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dW1xPyXPoonO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class linear_regression:\n",
    "    def __init__(self, learning_rate, iterations, \n",
    "            fit_intercept=True, normalize=False, coef=None):\n",
    "\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.normalize = normalize\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.coef = coef\n",
    "\n",
    "  \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit linear model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array_like, shape (n_samples, n_targets)\n",
    "            Target values.\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        In a case that the fit_intercept parameter is set to true, \n",
    "        then only we have to take into consideration the normalization else we simply ignore it.\n",
    "        \"\"\"\n",
    "        if self.fit_intercept and self.normalize:\n",
    "            X = self.do_normalization(X)\n",
    "        X = self.check_fitIntercept(X) # checks if the intercept is to be added or not.\n",
    "        self.coef = np.zeros(X.shape[1]) # making the shapes of X and self.coef similar\n",
    "        m = y.shape[0] # number of samples\n",
    "        \n",
    "        \"\"\"\n",
    "        Logic to implement the gradient descent\n",
    "        Here, a combined code implementing the following lines has been added.\n",
    "        hypothesis : (h(theta) = theta0+theta1*x)\n",
    "        Error : (h(theta)) - y\n",
    "        theta(t+1) : theta(t) - (learning_rate/m) * sum(error*X) \n",
    "        \"\"\"\n",
    "        for i in range(self.iterations):\n",
    "            self.coef = self.coef - ((self.learning_rate/m) * X.T.dot((X.dot(self.coef) - y)))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        \"\"\"Predict using the linear model\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array_like, shape (n_samples, n_features)\n",
    "            Samples.\n",
    "        Returns\n",
    "        -------\n",
    "        C : array, shape (n_samples,)\n",
    "            Returns predicted values.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Here I check if normalization is to be done on the data based \n",
    "        on whether fit_intercept parameter is set to true or not.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self.fit_intercept and self.normalize:\n",
    "            self.do_normalization(X)\n",
    "        X = self.check_fitIntercept(X)\n",
    "        #Use the generated co-efficients to generate the values on the samples from the test set.\n",
    "        self.coef = self.coef.reshape(self.coef.shape[0],1)\n",
    "        return X.dot(self.coef)\n",
    "\n",
    "    \n",
    "    def check_fitIntercept(self,X):\n",
    "        \n",
    "        \"\"\"Check if the fit intercept parameter is set to true\n",
    "        If yes, \n",
    "        ----------\n",
    "        X : array_like, shape (n_samples, n_features)\n",
    "            Samples.\n",
    "        Returns\n",
    "        -------\n",
    "        X : array_like, shape (n_samples,n_features+1)\n",
    "            If fit_intercept is set to true, I concatenate an extra column of ones to accommodate the y-intercept  \n",
    "            If fit_intercept is set to false, I concatenate an extra column of zeros to accommodate the y-intercept  \n",
    "        \"\"\"\n",
    "        if self.fit_intercept:\n",
    "            ones = np.ones((X.shape[0],1), dtype=int)\n",
    "            X = np.concatenate((ones,X),axis = 1)#np.c_[ones,X]\n",
    "            \n",
    "        else:\n",
    "            zeros = np.zeros((X.shape[0],1), dtype=int)\n",
    "            X = np.concatenate((zeros,X),axis = 1)\n",
    "        return X\n",
    "\n",
    "    def do_normalization(self,X):\n",
    "        \"\"\"\n",
    "        Do the mean normalization on the data Using the formula:\n",
    "        X = (X- Mean(X))/Range(X)\n",
    "        where mean(X) gives us the mean of all the columns\n",
    "        Range(X) is the maximum-minimum in every column.\n",
    "        ------------\n",
    "        X : array_like, shape (n_samples, n_features) \n",
    "        \n",
    "        Returns\n",
    "        ---------\n",
    "        X : array_like, shape (n_samples,n_features)\n",
    "        \"\"\"\n",
    "        return ((X - np.mean(X,axis=0))/np.ptp(X, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Man3c1JrhVbr"
   },
   "source": [
    "## Problem 1.2 (10 points)\n",
    "\n",
    "- Split the Boston Housing dataset into train and test sets (70% and 30%, respectively) (5 points). \n",
    "- Fit your linear regression implementation using the training set and print your model's coefficients. Make predictions for the test set using your fitted model (5 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VEFBL6WwhXUz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-efficient for Linear Regresion with one variable\n",
      "[[34.6418136 ]\n",
      " [-0.95534154]]\n",
      "Predicted the test data  results for Linear regression with one variable:\n",
      "[[24.81134913]\n",
      " [23.06307411]\n",
      " [31.24079771]\n",
      " [29.21547364]\n",
      " [ 5.37970218]\n",
      " [ 1.76851115]\n",
      " [23.77002685]\n",
      " [29.01485192]\n",
      " [24.98331061]\n",
      " [21.17149786]\n",
      " [29.54984318]\n",
      " [25.55651554]\n",
      " [26.90354711]\n",
      " [22.56629651]\n",
      " [12.43967617]\n",
      " [25.90999191]\n",
      " [14.14018411]\n",
      " [12.78359912]\n",
      " [ 6.88914181]\n",
      " [18.93599865]\n",
      " [28.19325819]\n",
      " [21.80202328]\n",
      " [26.90354711]\n",
      " [20.64606001]\n",
      " [20.29258364]\n",
      " [23.47387097]\n",
      " [32.98907273]\n",
      " [28.31745259]\n",
      " [30.04662078]\n",
      " [27.16148933]\n",
      " [21.17149786]\n",
      " [26.98952785]\n",
      " [29.83644564]\n",
      " [30.1803686 ]\n",
      " [15.75471132]\n",
      " [18.73537693]\n",
      " [12.09575321]\n",
      " [21.96443134]\n",
      " [24.98331061]\n",
      " [24.83045596]\n",
      " [25.56606895]\n",
      " [14.56053439]\n",
      " [29.19636681]\n",
      " [ 7.6247548 ]\n",
      " [24.38144544]\n",
      " [20.14928241]\n",
      " [31.89998337]\n",
      " [18.79269742]\n",
      " [28.04995696]\n",
      " [ 1.80672481]\n",
      " [28.03085013]\n",
      " [20.5123122 ]\n",
      " [ 5.41791584]\n",
      " [30.25679592]\n",
      " [23.0535207 ]\n",
      " [22.41344186]\n",
      " [24.55340692]\n",
      " [20.21615632]\n",
      " [19.06974647]\n",
      " [19.32768868]\n",
      " [19.50920358]\n",
      " [29.32056121]\n",
      " [19.41366942]\n",
      " [27.44809179]\n",
      " [30.00840712]\n",
      " [25.64249627]\n",
      " [31.00196232]\n",
      " [25.15527209]\n",
      " [14.72294245]\n",
      " [26.77935271]\n",
      " [ 8.16929948]\n",
      " [ 4.08043768]\n",
      " [25.9673124 ]\n",
      " [20.54097244]\n",
      " [25.52785529]\n",
      " [28.45120041]\n",
      " [23.14905485]\n",
      " [23.51208464]\n",
      " [24.85911621]\n",
      " [25.45142797]\n",
      " [32.75023734]\n",
      " [19.78625262]\n",
      " [22.01219842]\n",
      " [30.01796053]\n",
      " [31.65159457]\n",
      " [27.19970299]\n",
      " [22.11728599]\n",
      " [26.00552606]\n",
      " [28.84289044]\n",
      " [21.14283761]\n",
      " [26.24436145]\n",
      " [24.84000938]\n",
      " [14.5032139 ]\n",
      " [31.6707014 ]\n",
      " [31.62293432]\n",
      " [11.3410334 ]\n",
      " [30.89687475]\n",
      " [30.39054374]\n",
      " [19.63339798]\n",
      " [17.12084972]\n",
      " [30.45741764]\n",
      " [27.7633545 ]\n",
      " [28.07861721]\n",
      " [26.26346828]\n",
      " [22.28924746]\n",
      " [30.79178718]\n",
      " [18.86912474]\n",
      " [18.45832788]\n",
      " [20.91355564]\n",
      " [17.38834535]\n",
      " [20.77025441]\n",
      " [23.6744927 ]\n",
      " [17.31191803]\n",
      " [ 9.06732053]\n",
      " [30.28545617]\n",
      " [25.5374087 ]\n",
      " [30.28545617]\n",
      " [21.89755743]\n",
      " [26.5882844 ]\n",
      " [24.80179572]\n",
      " [29.39698853]\n",
      " [22.9293263 ]\n",
      " [29.38743512]\n",
      " [ 2.14109435]\n",
      " [31.76623556]\n",
      " [24.70626156]\n",
      " [16.34702307]\n",
      " [23.44521073]\n",
      " [25.11705843]\n",
      " [31.21213746]\n",
      " [24.25725104]\n",
      " [29.6262705 ]\n",
      " [24.65849449]\n",
      " [ 9.37302982]\n",
      " [22.13639282]\n",
      " [ 6.41147104]\n",
      " [14.35035925]\n",
      " [13.51921211]\n",
      " [19.02197939]\n",
      " [25.26991307]\n",
      " [14.44589341]\n",
      " [17.64628757]\n",
      " [28.22191844]\n",
      " [27.94486939]\n",
      " [19.13662038]\n",
      " [20.19704949]\n",
      " [18.10485151]\n",
      " [13.05109476]\n",
      " [28.91931776]\n",
      " [15.10507907]\n",
      " [31.62293432]\n",
      " [20.62695318]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#loading the boston data set\n",
    "boston = load_boston()\n",
    "df = pd.DataFrame(boston.data,columns=boston.feature_names)\n",
    "df['target'] = boston.target\n",
    "y = boston.target\n",
    "#feature_list = df[['LSTAT','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B']]\n",
    "#print(boston.feature_names)\n",
    "feature_names = boston.feature_names\n",
    "\n",
    "\"\"\"\n",
    "For Linear Regression with Single Variable\n",
    "\n",
    "\"\"\"\n",
    "X = df[['LSTAT']].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.7, random_state=2)\n",
    "learning_rate = 0.009\n",
    "iterations = 10000\n",
    "fit_intercept = True\n",
    "normalization = False\n",
    "coeff = []\n",
    "lr = linear_regression(learning_rate, iterations, fit_intercept, normalization, coeff)\n",
    "lr.fit(X_train,y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"Co-efficient for Linear Regresion with one variable\")\n",
    "print(lr.coef)\n",
    "print(\"Predicted the test data  results for Linear regression with one variable:\")\n",
    "print(y_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-efficient for Multi-variate Linear Regresion:\n",
      "[[ 22.57288136]\n",
      " [-24.28177644]\n",
      " [ 24.50861214]]\n",
      "Predicted the test data  results for multi variate linear regression:\n",
      "[[ -67.66200741]\n",
      " [-148.88508511]\n",
      " [ 111.31731676]\n",
      " [  45.5024126 ]\n",
      " [-614.1243133 ]\n",
      " [-689.95432175]\n",
      " [ -84.71783667]\n",
      " [  49.61847771]\n",
      " [ -81.81979842]\n",
      " [-169.758443  ]\n",
      " [  53.58438795]\n",
      " [ -47.44777395]\n",
      " [ -30.0478857 ]\n",
      " [-162.46744474]\n",
      " [-378.7778411 ]\n",
      " [ -41.99275681]\n",
      " [-347.44295591]\n",
      " [-395.08420318]\n",
      " [-525.12431384]\n",
      " [-223.41618891]\n",
      " [  13.32023294]\n",
      " [-153.36484137]\n",
      " [   2.49955121]\n",
      " [-176.88823256]\n",
      " [-185.01468842]\n",
      " [-127.56209745]\n",
      " [ 164.11040439]\n",
      " [  40.10316598]\n",
      " [  83.44046604]\n",
      " [ -19.74198841]\n",
      " [-181.37552515]\n",
      " [ -23.96565649]\n",
      " [  67.09410837]\n",
      " [  63.3361557 ]\n",
      " [-326.38127262]\n",
      " [-243.78422733]\n",
      " [-414.79736592]\n",
      " [-142.91371744]\n",
      " [ -75.37403343]\n",
      " [ -76.3180842 ]\n",
      " [ -46.59224088]\n",
      " [-351.31708989]\n",
      " [  57.66322094]\n",
      " [-550.10169281]\n",
      " [ -85.23064069]\n",
      " [-193.99983233]\n",
      " [ 128.80700087]\n",
      " [-227.84273097]\n",
      " [  22.71654813]\n",
      " [-698.56591804]\n",
      " [  22.67206762]\n",
      " [-164.65118672]\n",
      " [-586.56119807]\n",
      " [  81.89553684]\n",
      " [-134.52077004]\n",
      " [-129.0504213 ]\n",
      " [ -89.36440935]\n",
      " [-207.59348196]\n",
      " [-228.03105638]\n",
      " [-224.34248436]\n",
      " [-220.14559324]\n",
      " [  32.8065082 ]\n",
      " [-222.76983978]\n",
      " [   7.59058925]\n",
      " [  71.75893147]\n",
      " [ -41.2920189 ]\n",
      " [ 122.52544421]\n",
      " [ -49.95041585]\n",
      " [-351.60073808]\n",
      " [ -19.11206466]\n",
      " [-504.44890168]\n",
      " [-631.65808639]\n",
      " [ -47.91294248]\n",
      " [-195.80843782]\n",
      " [ -46.55865884]\n",
      " [  31.73848085]\n",
      " [-125.4262499 ]\n",
      " [-113.72380503]\n",
      " [ -84.24117099]\n",
      " [ -54.43228509]\n",
      " [ 146.64345564]\n",
      " [-202.34459734]\n",
      " [-146.25823047]\n",
      " [  70.97238753]\n",
      " [ 143.62016266]\n",
      " [  13.13949565]\n",
      " [-143.09706282]\n",
      " [ -39.68712223]\n",
      " [  43.58117233]\n",
      " [-179.3835225 ]\n",
      " [ -46.63075116]\n",
      " [ -77.15364537]\n",
      " [-328.87809964]\n",
      " [ 129.74375148]\n",
      " [ 128.92180045]\n",
      " [-425.15686895]\n",
      " [ 115.76151057]\n",
      " [  89.38792377]\n",
      " [-214.5626097 ]\n",
      " [-284.59985201]\n",
      " [  78.07357508]\n",
      " [   5.23643255]\n",
      " [  12.02398817]\n",
      " [ -47.00291706]\n",
      " [-138.70183445]\n",
      " [ 100.81170048]\n",
      " [-241.09552838]\n",
      " [-243.27728996]\n",
      " [-182.4906929 ]\n",
      " [-282.28603062]\n",
      " [-177.99610014]\n",
      " [-106.01764566]\n",
      " [-284.89030527]\n",
      " [-480.71721318]\n",
      " [  72.52644194]\n",
      " [ -56.48691511]\n",
      " [  82.79555042]\n",
      " [-157.45595455]\n",
      " [ -35.11983847]\n",
      " [ -79.96306234]\n",
      " [  66.73278916]\n",
      " [-120.08021747]\n",
      " [  42.20193677]\n",
      " [-658.03454022]\n",
      " [ 136.85307404]\n",
      " [ -82.68534333]\n",
      " [-291.15598343]\n",
      " [-114.39416767]\n",
      " [ -75.69989377]\n",
      " [  93.18774885]\n",
      " [ -96.98979449]\n",
      " [  76.75138818]\n",
      " [ -77.89482218]\n",
      " [-468.90312371]\n",
      " [-158.174396  ]\n",
      " [-562.31300367]\n",
      " [-326.66053945]\n",
      " [-371.7060904 ]\n",
      " [-230.15196385]\n",
      " [ -66.10430291]\n",
      " [-358.64245325]\n",
      " [-253.99081202]\n",
      " [  16.67110773]\n",
      " [  25.41293878]\n",
      " [-217.09158525]\n",
      " [-194.35429469]\n",
      " [-256.64858881]\n",
      " [-386.17756513]\n",
      " [  39.73968198]\n",
      " [-317.69802732]\n",
      " [ 140.02420175]\n",
      " [-186.83419238]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For Multi-variate Linear regression\n",
    "\n",
    "\"\"\"\n",
    "X = df[['LSTAT',\"RM\"]].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.7, random_state=2)\n",
    "learning_rate = 0.007\n",
    "iterations = 100000\n",
    "fit_intercept = True\n",
    "normalization = True\n",
    "coeff = []\n",
    "lr = linear_regression(learning_rate, iterations, fit_intercept, normalization, coeff)\n",
    "lr.fit(X_train,y_train)\n",
    "# predictions are made here using our new co-efficients\n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"Co-efficient for Multi-variate Linear Regresion:\") \n",
    "print(lr.coef)\n",
    "print(\"Predicted the test data  results for multi variate linear regression:\")\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9FXqtD_KhZwV"
   },
   "source": [
    "## Problem 1.3 (10 points)\n",
    "\n",
    "Identify the variable or set of variables that will minimize the mean square error (MSE). Hint: this is where your function being able to handle simple and multiple regression becomes useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sFlcvY_piKus"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error is :  [49.492346133734195]\n",
      "The variables that reduce the Mean squared error are: ['CRIM', 'CHAS', 'RM']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import itertools as it\n",
    "# Your code goes here\n",
    "v = []\n",
    "learning_rate = 0.007\n",
    "iterations = 10\n",
    "fit_intercept = True\n",
    "normalization = False\n",
    "coeff = []\n",
    "lr = linear_regression(learning_rate, iterations, fit_intercept, normalization, coeff)\n",
    "# list of all the combinations of the features available to us\n",
    "for i in range(1,len(feature_names)):\n",
    "     v.append(list(it.combinations(feature_names.tolist(),i)))\n",
    "mse_list = []\n",
    "\n",
    "# iterating through each combination to find the least MSE\n",
    "for i in v:\n",
    "    for j in i:\n",
    "        X = df[list(j)].values\n",
    "        columns = list(j)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.7, random_state=2)\n",
    "        lr.fit(X_train,y_train)\n",
    "        y_pred = lr.predict(X_test)\n",
    "        mse = mean_squared_error(y_test,y_pred)\n",
    "        if len(mse_list) == 0:\n",
    "            mse_list.append(mse)\n",
    "            mse_index = columns\n",
    "            \n",
    "        if mse_list[0]>mse:\n",
    "            mse_list.pop()\n",
    "            mse_list.append(mse)\n",
    "            mse_index = columns\n",
    "\n",
    "print(\"The mean squared error is : \",mse_list)        \n",
    "print(\"The variables that reduce the Mean squared error are:\", mse_index )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KcqYa1U3iRQ1"
   },
   "source": [
    "## Problem 1.4 (5 points)\n",
    "\n",
    "1. How do you interpret that a variable causes a model's mean square error to increase? (2 points)\n",
    "  - Answer: If the difference between the predicted value and the actual value is high, the square of the difference increases.  Mean squared error glorifies the  differences, as values are squared. If the values are scattered away from the model, the likelihood of them increasing the mean squared error is huge. \n",
    "2. Why we would want to normalize our variables? (1 point)\n",
    "  - Answer: We can see in the data all the different features have different ranges. This, creates a problem where in some features dominate the answer whlist the others do not contribute that much. In that case,Gradient descent will work slowly to reach the global minima as it will have wide range of features to update from.\n",
    " \n",
    "3. A model fitted using the exact same split dataset with normalized values will generate the same coefficients as a model that was fitted using values that haven't been normalized. Clearly state whether that statement is true or false and explain your reasoning. (2 points)\n",
    "  - Answer:No, the state is false. The model wont produce the same coefficients after being normalized since the ranges are different. After the normalization, we normally change the scale, therefore the corresponding coefficients that we get/update after normalization will be different from the original values.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RJSot41BkMrB"
   },
   "source": [
    "# Problem 2: Binary Classification\n",
    "\n",
    "## Problem 2.1 (5 points)\n",
    "\n",
    "Consider the binary classification problem of mapping a given input to two classes. Let $\\mathcal{X}=\\mathbb{R}^d$ and $\\mathcal{Y}=\\{+1, -1\\}$ be the input space and output space, respectively. In simple words, it means that the input has $d$ features and all of them are real valued, whereas the output can only take values $-1$ or $+1$. This is one of the most common problems in machine learning and many sophisticated methods exist to solve it. In the question, we will solve it using the concepts we have already learned in class. Let us assume the two sets of points can be separated using a straight line i.e. the samples are linearly separable. So if $d=2$, one should be able to draw a line to distinguish between the two classes. All points lying on side of the line should belong to a particular class (say $1$) and the points lying on the other side should belong to another class (say $2$). To see what this would look like,  your first task is as follows:\n",
    "\n",
    "Write a function that will randomly generate a dataset for this problem. Your function should randomly choose a line $l$, which can be denoted as $ax + by + c = 0$. According to basic high school geometry, the line divides the plane into two sides. On one side, $ax+by+c>0$ while on the other $ax+by+c<0$. Use this fact to randomly generate $k_0$ points on the side of class 0 (i.e. $y=-1$) and $k_1$ points on the side of class 1 (i.e. $y=1$). Create a plot of this dataset where all the points corresponding to one class are blue and those of the other class are green, the line dividing both classes should be red. Axes should be labeled.\n",
    "\n",
    "**Note**: Do not confuse the $x$ and $y$ in the equation of line $ax + by + c = 0$ with $\\mathcal{X} $ and $\\mathcal{Y}$. Instead imagine these $x$ and $y$ as the 2-D coordinate system on which you have different points which should lie on 2 sides of the line $ax + by + c = 0$. For example, there is a point (2,3) in the 2-D system where $x = 2$ and $y = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g96jFpGyMIFu"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random as rand\n",
    "import numpy as np\n",
    "def generate_dataset(k0, k1):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    k0 : integer, number of samples for class 0\n",
    "    k1 : integer, number of samples for class 1\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : array, shape (m, d), dimension numpy array where m is the number of \n",
    "    samples and d is the number of features \n",
    "\n",
    "    Y : array, (m, 1), dimension vector where m is the number of samples\n",
    "    \n",
    "    \"\"\"\n",
    "    m = k0+k1 # total number of points\n",
    "    # co-ordinators of points\n",
    "    a = rand.randint(1,10)  \n",
    "    b = rand.randint(1,10) \n",
    "    c = rand.randint(1,10)\n",
    "    # shape of X\n",
    "    d = 2\n",
    "   \n",
    "    X = []\n",
    "    y =[] \n",
    "    k0_l = [] #\n",
    "    k1_l = []\n",
    "    s = 0\n",
    "    # collecting points for the k0 set\n",
    "    while s<k0:\n",
    "        \n",
    "        rx = rand.randint(-10,10)\n",
    "        ry = rand.randint(-10,10)\n",
    "        if  (rx*a+ry*b+c) < 0:\n",
    "            X.append([rx,ry])\n",
    "            y.append([-1])\n",
    "            k0_l.append([rx,ry])\n",
    "            s+=1\n",
    "    s=0\n",
    "    # collecting points for the k1 set\n",
    "    while s<k1:\n",
    "        rx = rand.randint(-10,10)#(rand.randint(1,100),rand.randint(1,100))\n",
    "        ry = rand.randint(-10,10)\n",
    "        if  (rx*a+ry*b+c)  > 0:\n",
    "            X.append([rx,ry])\n",
    "            y.append([1])\n",
    "            k1_l.append([rx,ry])\n",
    "            s+=1\n",
    "        \n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    k0_l = np.asarray(k0_l)\n",
    "    k1_l = np.asarray(k1_l)\n",
    "    j = 0\n",
    "    points_x = []\n",
    "    points_y = []\n",
    "    for i in range(-10,10):\n",
    "        points_x.append(i)\n",
    "        points_y.append((-(c/b)-((a*i)/b))) #obtaining y values by substituting x in the ax+by+c=0 format\n",
    "    #plot the points\n",
    "    plt.plot(points_x,points_y, 'r-')\n",
    "    plt.scatter(k0_l[:,0],k0_l[:,1],color='blue')\n",
    "    plt.scatter(k1_l[:,0],k1_l[:,1],color='green')\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    return X,y\n",
    "\n",
    "X,y= generate_dataset(100, 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9mw15CFikXI1"
   },
   "source": [
    "## Problem 2.2 (35 points)\n",
    "\n",
    "If $\\mathcal{Y}$ is the variable you are trying to predict using a feature $\\mathcal{X}$ then in a typical Machine Learning problem, you are tasked with a target function $f$ which maps $\\mathcal{X}$ to $\\mathcal{Y}$ i.e. Find $f$ such that  $\\mathcal{Y}$  = $f(\\mathcal{X})$\n",
    "\n",
    "\n",
    "When you are given a dataset for which you do not have access the target function $f$, you have to learn it from the data. In this problem, we are going to learn the parameters of the line that separates the two classes for the dataset that we constructed in Problem 2.1. As we previously mentioned, that line can be represented as $ax + by + c = 0$.\n",
    "\n",
    "The goal here is to correctly find out the coefficients $a$, $b$, and $c$, represented below as $\\bf{w}$ which is a vector. The algorithm to find it is a simple iterative process: \n",
    "\n",
    "1. Randomly choose a $\\mathbf{w}$ to begin with.\n",
    "2. Keep on adjusting the value of $\\bf{w}$ as follows until all data samples are correctly classified:\n",
    "    1. Randomly choose a sample from the dataset without replacement and see if it is correctly classified. If yes,  move on to another sample.\n",
    "    2. If not,  then  update the weights as $\\mathbf{w}^{t+1} = \\mathbf{w}^t + y \\cdot \\mathbf{x}$\n",
    "    and go back to the previous step (of randomly chosing a sample)\n",
    "    \n",
    "        - $\\mathbf{w}^{t+1}$ is value of $\\mathbf{w}$ at iteration $t+1$\n",
    "        - $\\mathbf{w}^{t}$ is value of $\\mathbf{w}$ at iteration $t$\n",
    "        - $y$ is the class label for the sample under consideration\n",
    "        - $\\mathbf{x}$ is the data-point under consideration\n",
    "    \n",
    "    \n",
    "Write a function that implements this learning algorithm. The input to the function is going to be a dataset represented by the input variable $X$ and the target variable $y$. The output of the function should be the chosen $\\mathbf{w}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SPk7AZaLkXSh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The new coefficients of the line are [[ 7.  9. -1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "            \n",
    "def fit_line(X,y):\n",
    "    \n",
    "    \"\"\"\n",
    "    This method learns the data set and generates co-efficients to provide us \n",
    "    with the slope and the intercept of the line that should seperate the data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "     X : array, shape (m, d), dimension numpy array where m is the number of \n",
    "    samples and d is the number of features \n",
    "\n",
    "    Y : array, (m, 1), dimension vector where m is the number of samples\n",
    "    \n",
    "    -----------\n",
    "    Returns : the coefficients for the binary classification\n",
    "    \"\"\"\n",
    "    # intercept fit on the X array\n",
    "    ones = np.ones((X.shape[0],1))\n",
    "    X = np.c_[X,ones]\n",
    "\n",
    "    randomList =  random.sample(range(0,X.shape[0]), X.shape[0])\n",
    "    track_array = []\n",
    "    w = np.random.randint(2,size=(1,3))\n",
    "    \"\"\"\n",
    "    The steps provided in the algorithm above are performed here.\n",
    "    \"\"\"\n",
    "    while(True):\n",
    "        if len(track_array) == X.shape[0]:\n",
    "            break\n",
    "        randomInt = random.choice(randomList)\n",
    "        if randomInt not in track_array: \n",
    "            X_val = X[randomInt]\n",
    "            y_val = y[randomInt]\n",
    "            y_predicted = X_val[0]*w[0][0]+w[0][1]*X_val[1]+w[0][2]\n",
    "            if y_predicted < 0:\n",
    "                if y_val == [-1]:\n",
    "                    track_array.append(randomInt)\n",
    "                    \n",
    "                else:\n",
    "                    updated_val = X_val * y_val\n",
    "                    w = w + updated_val\n",
    "                    # if the prediction goes wrong, the points that were already classified must be dicarded.\n",
    "                    track_array.clear()\n",
    "            else:\n",
    "                if y_val == [1]:\n",
    "                    track_array.append(randomInt)\n",
    "                else:\n",
    "                    updated_val = X_val * y_val\n",
    "                    w = w + updated_val\n",
    "                    # if the prediction goes wrong, the points that were already classified must be dicarded.\n",
    "                    track_array.clear()\n",
    "\n",
    "    return w\n",
    "#output \n",
    "w = fit_line(X,y)\n",
    "points_x1 = []\n",
    "points_y1 = []\n",
    "k0_list = []\n",
    "k1_list = []\n",
    "# plot the original points and the learned line  \n",
    "for i in range(X.shape[0]):\n",
    "    if y[i] == -1:\n",
    "        k0_list.append(list(X[i]))\n",
    "    else:\n",
    "        k1_list.append(list(X[i]))\n",
    "k0_list = np.asarray(k0_list)\n",
    "k1_list = np.asarray(k1_list)\n",
    "# generating points that fit the equation with the above obtained slope and intercept\n",
    "for i in range(-10,10):\n",
    "        points_x1.append(i)\n",
    "        points_y1.append((-(w[0][2]/w[0][1])-((w[0][0]*i)/w[0][1])))\n",
    "plt.plot(points_x1,points_y1, 'y-')\n",
    "plt.scatter(k0_list[:,0],k0_list[:,1],color=\"blue\")\n",
    "plt.scatter(k1_list[:,0],k1_list[:,1],color=\"green\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "print(\" The new coefficients of the line are\",w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHc3t4e9MOxu"
   },
   "source": [
    "### Problem 2.3 (10 points)\n",
    "- Give an intuition of why the above algorithm converges for linearly separable data? We do not expect you to give a mathematic proof, but it would be great if you can provide one. You will get full points even if you just provide an intuition of a few lines. Including figures or mathematical equations is encouraged but not required. (5 points)\n",
    "\n",
    "  - Answer:In the above algorithm, whenever we get an incorrect assumption, we update the value of the coefficient. In other words we change the slope and the y intercept of the line. So whenever we get a wrong assumption, we use the X and y co-ordinates and update the values of co-efficient. Thus, when we get a point which lies on the side of the line for which ax+by+c < 0, and if we update the current slope by adding the co-ordinates of the points, we are basically reducing the slope and thus the y co-ordinate will increase and the x will decrease. Likewise when we get a point which lies on the side of the line where ax+by+c > 0, we see that the slope increases. This keeps on changing, until we sort all the points and then reach a state where we obtain those co-efficients which linearly separate the data.\n",
    "- What happens when the data is not linearly separable? What can be done to salvage the situation?\n",
    "\n",
    "  - Answer: If the data is not linearly separable we can increase the degree of the polynomial. Ex: Linear regression with 1 variable gives us a straight line. H(x) = θ0+θ1*x\n",
    "  If there is some data that is not linearly separable using the above hypothesis, then we increase the degree of the\n",
    "  polynomial. For Example H(X) = θ0 + θ1 * x + θ2 * x^2+...... and so on and so forth. Increasing the ploynomial degree will\n",
    "  fit the data correctly. The bias variance tradeoff is the most needed here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "A1-F19.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
